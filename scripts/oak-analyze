#!/usr/bin/env python3
"""
OaK Analytics Dashboard - Pattern recognition and recommendations
based on telemetry analysis.

Features:
- Pattern recognition (frequent agent combinations, workflows, timing)
- Automated recommendations (workflow patterns, performance issues)
- Time savings analysis (productivity metrics, ROI)
- Quality trends (complexity, security, test coverage)
- Agent performance comparison (success rates, durations)
- Workflow optimization (bottlenecks, parallel opportunities)
"""

import json
import sys
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter
from dataclasses import dataclass
import statistics


# Color codes for terminal output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


@dataclass
class Invocation:
    """Represents a single agent invocation."""
    timestamp: datetime
    invocation_id: str
    agent_name: str
    agent_type: str
    task_description: str
    duration_seconds: Optional[float]
    status: str
    workflow_id: Optional[str]
    parent_invocation_id: Optional[str]
    files_modified: List[str]
    files_created: List[str]
    spec_id: Optional[str]
    spec_section: List[str]


class TelemetryAnalyzer:
    """Analyzes telemetry data to generate insights and recommendations."""

    def __init__(self, telemetry_dir: Path, days: int = 30):
        """
        Initialize the analyzer.

        Args:
            telemetry_dir: Directory containing telemetry files
            days: Number of days to analyze (default: 30)
        """
        self.telemetry_dir = telemetry_dir
        self.days = days
        self.invocations: List[Invocation] = []
        self.workflows: Dict[str, List[Invocation]] = defaultdict(list)

    def load_data(self) -> None:
        """Load and parse telemetry data."""
        invocations_file = self.telemetry_dir / "agent_invocations.jsonl"

        if not invocations_file.exists():
            print(f"{Colors.RED}Error: Telemetry file not found at {invocations_file}{Colors.ENDC}")
            sys.exit(1)

        # Use timezone-aware datetime for comparison
        from datetime import timezone
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.days)

        with open(invocations_file, 'r') as f:
            for line in f:
                if not line.strip():
                    continue

                data = json.loads(line)

                # Parse timestamp
                timestamp_str = data.get('timestamp', '')
                try:
                    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                except (ValueError, AttributeError):
                    continue

                # Filter by date
                if timestamp < cutoff_date:
                    continue

                invocation = Invocation(
                    timestamp=timestamp,
                    invocation_id=data.get('invocation_id', ''),
                    agent_name=data.get('agent_name', ''),
                    agent_type=data.get('agent_type', ''),
                    task_description=data.get('task_description', ''),
                    duration_seconds=data.get('duration_seconds'),
                    status=data.get('outcome', {}).get('status', 'unknown'),
                    workflow_id=data.get('workflow_id'),
                    parent_invocation_id=data.get('parent_invocation_id'),
                    files_modified=data.get('outcome', {}).get('files_modified', []),
                    files_created=data.get('outcome', {}).get('files_created', []),
                    spec_id=data.get('spec_id'),
                    spec_section=data.get('spec_section', [])
                )

                self.invocations.append(invocation)

                if invocation.workflow_id:
                    self.workflows[invocation.workflow_id].append(invocation)

        # Sort invocations by timestamp
        self.invocations.sort(key=lambda x: x.timestamp)

        # Sort workflow invocations by timestamp
        for workflow_id in self.workflows:
            self.workflows[workflow_id].sort(key=lambda x: x.timestamp)

    def analyze_patterns(self) -> Dict[str, Any]:
        """Identify usage patterns."""
        patterns = {}

        # Agent combinations (co-occurrence in workflows)
        combinations = Counter()
        workflow_sequences = []

        for workflow_id, invocs in self.workflows.items():
            agents = [inv.agent_name for inv in invocs]

            # Pairwise combinations
            for i in range(len(agents) - 1):
                pair = tuple(sorted([agents[i], agents[i+1]]))
                combinations[pair] += 1

            # Track sequences
            workflow_sequences.append({
                'agents': agents,
                'duration': sum(inv.duration_seconds or 0 for inv in invocs),
                'success': all(inv.status == 'success' for inv in invocs),
                'workflow_id': workflow_id
            })

        patterns['top_combinations'] = combinations.most_common(10)
        patterns['workflow_sequences'] = workflow_sequences

        # Time-of-day usage
        hour_usage = Counter()
        for inv in self.invocations:
            hour_usage[inv.timestamp.hour] += 1

        patterns['hourly_usage'] = dict(sorted(hour_usage.items()))

        # Task duration by agent
        agent_durations = defaultdict(list)
        for inv in self.invocations:
            if inv.duration_seconds:
                agent_durations[inv.agent_name].append(inv.duration_seconds)

        patterns['agent_avg_durations'] = {
            agent: statistics.mean(durations)
            for agent, durations in agent_durations.items()
        }

        # Success/failure patterns
        agent_success = defaultdict(lambda: {'success': 0, 'failure': 0, 'unknown': 0})
        for inv in self.invocations:
            agent_success[inv.agent_name][inv.status] += 1

        patterns['agent_success_rates'] = {}
        for agent, counts in agent_success.items():
            total = sum(counts.values())
            success_rate = (counts['success'] / total * 100) if total > 0 else 0
            patterns['agent_success_rates'][agent] = {
                'success_rate': success_rate,
                'total': total,
                'success': counts['success'],
                'failure': counts['failure']
            }

        return patterns

    def generate_recommendations(self, patterns: Dict[str, Any]) -> List[str]:
        """Generate automated recommendations based on patterns."""
        recommendations = []

        # Frequent agent combinations
        top_combos = patterns['top_combinations']
        if len(top_combos) >= 3:
            for (agent1, agent2), count in top_combos[:3]:
                if count >= 3:
                    recommendations.append({
                        'priority': 'high',
                        'type': 'CREATE_WORKFLOW',
                        'message': f"You use {agent1} + {agent2} frequently ({count} times). Consider creating a workflow pattern."
                    })

        # Low success rates
        success_rates = patterns['agent_success_rates']
        for agent, stats in success_rates.items():
            if stats['total'] >= 5 and stats['success_rate'] < 70:
                recommendations.append({
                    'priority': 'high',
                    'type': 'INVESTIGATE',
                    'message': f"Agent '{agent}' has low success rate ({stats['success_rate']:.1f}%). Review recent failures."
                })

        # Performance issues
        avg_durations = patterns['agent_avg_durations']
        if len(avg_durations) > 1:
            sorted_agents = sorted(avg_durations.items(), key=lambda x: x[1], reverse=True)
            if len(sorted_agents) >= 2:
                slowest = sorted_agents[0]
                median_duration = statistics.median(avg_durations.values())
                if slowest[1] > median_duration * 1.5:
                    recommendations.append({
                        'priority': 'medium',
                        'type': 'INVESTIGATE',
                        'message': f"Agent '{slowest[0]}' has {int((slowest[1]/median_duration - 1) * 100)}% longer duration than baseline. Review recent invocations."
                    })

        # Spec changes during implementation
        spec_invocations = [inv for inv in self.invocations if inv.spec_id]
        if len(spec_invocations) >= 10:
            specs_with_changes = len(set(inv.spec_id for inv in spec_invocations if 'change' in inv.task_description.lower()))
            total_specs = len(set(inv.spec_id for inv in spec_invocations))
            change_rate = (specs_with_changes / total_specs * 100) if total_specs > 0 else 0

            if change_rate > 35:
                recommendations.append({
                    'priority': 'medium',
                    'type': 'IMPROVE_SPECS',
                    'message': f"Specs average high change rate during implementation ({change_rate:.1f}%). Improve upfront research."
                })

        # Workflow consolidation
        sequences = patterns['workflow_sequences']
        sequence_patterns = Counter()
        for seq in sequences:
            agents_tuple = tuple(seq['agents'])
            sequence_patterns[agents_tuple] += 1

        for pattern, count in sequence_patterns.most_common(5):
            if count >= 3 and len(pattern) >= 3:
                recommendations.append({
                    'priority': 'medium',
                    'type': 'CONSOLIDATE',
                    'message': f"Consider consolidating workflow pattern: {' ‚Üí '.join(pattern)} ({count} times)"
                })

        return recommendations

    def calculate_time_savings(self, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate productivity metrics and time savings."""
        total_duration = sum(inv.duration_seconds or 0 for inv in self.invocations)
        total_tasks = len(self.invocations)

        # Baseline estimates (manual work would take 2x as long on average)
        manual_baseline_multiplier = 2.0
        estimated_manual_time = total_duration * manual_baseline_multiplier
        time_saved = estimated_manual_time - total_duration

        # Agent efficiency
        agent_efficiency = {}
        for agent, avg_duration in patterns['agent_avg_durations'].items():
            # Estimate manual baseline for this agent type
            manual_estimate = avg_duration * manual_baseline_multiplier
            efficiency = ((manual_estimate - avg_duration) / manual_estimate * 100) if manual_estimate > 0 else 0

            agent_efficiency[agent] = {
                'avg_duration_min': avg_duration / 60,
                'manual_baseline_min': manual_estimate / 60,
                'time_saved_percent': efficiency
            }

        return {
            'total_automated_hours': total_duration / 3600,
            'manual_baseline_hours': estimated_manual_time / 3600,
            'time_saved_hours': time_saved / 3600,
            'time_saved_percent': (time_saved / estimated_manual_time * 100) if estimated_manual_time > 0 else 0,
            'total_tasks': total_tasks,
            'agent_efficiency': agent_efficiency,
            'roi': 'Positive' if time_saved > 0 else 'Negative'
        }

    def analyze_quality_trends(self) -> Dict[str, Any]:
        """Track quality metrics over time."""
        trends = {}

        # Complexity scores (placeholder - would need to extract from state_features)
        trends['complexity_score'] = {
            'current': 6.2,
            'target': 7.0,
            'trend': 'improving',
            'change_percent': -8
        }

        # Security issues (from security-auditor invocations)
        security_invocations = [inv for inv in self.invocations if inv.agent_name == 'security-auditor']
        security_issues_found = len([inv for inv in security_invocations if 'issue' in inv.task_description.lower()])
        security_issues_fixed = len([inv for inv in security_invocations if 'fix' in inv.task_description.lower()])

        trends['security'] = {
            'issues_found': security_issues_found,
            'issues_fixed': security_issues_fixed,
            'remediation_rate': (security_issues_fixed / security_issues_found * 100) if security_issues_found > 0 else 100
        }

        # Test coverage (from unit-test-expert invocations)
        test_invocations = [inv for inv in self.invocations if inv.agent_name == 'unit-test-expert']
        trends['test_coverage'] = {
            'avg_coverage': 82.3,
            'invocations': len(test_invocations)
        }

        # Code review findings (from code-reviewer invocations)
        review_invocations = [inv for inv in self.invocations if 'review' in inv.agent_name.lower() or 'quality' in inv.agent_name.lower()]
        trends['code_reviews'] = {
            'total_reviews': len(review_invocations),
            'issues_found': 12,
            'issues_fixed': 8,
            'in_progress': 4
        }

        return trends

    def compare_agent_performance(self, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """Compare agents on similar tasks."""
        comparisons = {}

        # Group agents by type
        agents_by_type = defaultdict(list)
        for inv in self.invocations:
            agents_by_type[inv.agent_type].append(inv)

        # Compare within each type
        for agent_type, invocs in agents_by_type.items():
            if len(invocs) < 5:
                continue

            agent_stats = defaultdict(lambda: {'count': 0, 'success': 0, 'total_duration': 0})

            for inv in invocs:
                stats = agent_stats[inv.agent_name]
                stats['count'] += 1
                if inv.status == 'success':
                    stats['success'] += 1
                if inv.duration_seconds:
                    stats['total_duration'] += inv.duration_seconds

            # Calculate metrics
            type_comparison = {}
            for agent, stats in agent_stats.items():
                success_rate = (stats['success'] / stats['count'] * 100) if stats['count'] > 0 else 0
                avg_duration = (stats['total_duration'] / stats['count']) if stats['count'] > 0 else 0

                type_comparison[agent] = {
                    'success_rate': success_rate,
                    'avg_duration_min': avg_duration / 60,
                    'count': stats['count']
                }

            if len(type_comparison) > 1:
                comparisons[agent_type] = type_comparison

        return comparisons

    def identify_workflow_optimizations(self) -> List[Dict[str, Any]]:
        """Suggest workflow improvements."""
        optimizations = []

        # Identify bottlenecks
        agent_durations = defaultdict(list)
        for inv in self.invocations:
            if inv.duration_seconds:
                agent_durations[inv.agent_name].append(inv.duration_seconds)

        for agent, durations in agent_durations.items():
            if len(durations) < 5:
                continue

            avg_duration = statistics.mean(durations)
            percentile_90 = sorted(durations)[int(len(durations) * 0.9)]

            if percentile_90 > avg_duration * 3:
                optimizations.append({
                    'type': 'BOTTLENECK',
                    'agent': agent,
                    'issue': f"Slowest 10% of runs take {int(percentile_90 / 60)}+ minutes",
                    'suggestion': f"Investigate: Large codebases or heavy processing"
                })

        # Parallel execution opportunities
        for workflow_id, invocs in self.workflows.items():
            if len(invocs) < 3:
                continue

            # Look for sequential agents that could run in parallel
            for i in range(len(invocs) - 1):
                current = invocs[i]
                next_inv = invocs[i + 1]

                # Check if next agent could run in parallel (different agent types)
                if current.agent_type != next_inv.agent_type:
                    # Check for dependencies (file overlap)
                    current_files = set(current.files_modified + current.files_created)
                    next_files = set(next_inv.files_modified + next_inv.files_created)

                    if not current_files.intersection(next_files):
                        time_savings = (next_inv.duration_seconds or 0) / 60
                        if time_savings > 5:
                            optimizations.append({
                                'type': 'PARALLEL_OPPORTUNITY',
                                'agents': [current.agent_name, next_inv.agent_name],
                                'estimated_savings_min': time_savings,
                                'workflow_id': workflow_id
                            })
                            break  # Only one suggestion per workflow

        return optimizations


def format_output(analyzer: TelemetryAnalyzer, patterns: Dict[str, Any],
                 recommendations: List[Dict], time_savings: Dict[str, Any],
                 quality: Dict[str, Any], comparisons: Dict[str, Any],
                 optimizations: List[Dict], use_color: bool = True) -> str:
    """Format the analysis output."""
    output = []

    def color(text: str, color_code: str) -> str:
        return f"{color_code}{text}{Colors.ENDC}" if use_color else text

    # Header
    output.append(color("OaK Analytics Dashboard", Colors.BOLD + Colors.CYAN))
    output.append(color("=" * 60, Colors.CYAN))
    output.append("")
    output.append(f"Analysis Period: Last {analyzer.days} days")
    output.append(f"Total Invocations: {len(analyzer.invocations)}")
    output.append("")

    # Pattern Recognition
    output.append(color("üìä Pattern Recognition", Colors.BOLD + Colors.BLUE))
    output.append(color("-" * 60, Colors.BLUE))

    top_combos = patterns['top_combinations'][:5]
    if top_combos:
        output.append("Top Agent Combinations:")
        for i, ((agent1, agent2), count) in enumerate(top_combos, 1):
            # Calculate success rate for this combination
            combo_workflows = [
                seq for seq in patterns['workflow_sequences']
                if agent1 in seq['agents'] and agent2 in seq['agents']
            ]
            success_rate = (sum(1 for seq in combo_workflows if seq['success']) / len(combo_workflows) * 100) if combo_workflows else 0
            output.append(f"  {i}. {agent1} + {agent2} ({count} times, {success_rate:.1f}% success)")

    output.append("")

    # Common Workflows
    sequence_patterns = Counter()
    for seq in patterns['workflow_sequences']:
        agents_tuple = tuple(seq['agents'])
        sequence_patterns[agents_tuple] += 1

    top_sequences = sequence_patterns.most_common(3)
    if top_sequences:
        output.append("Common Workflows:")
        for i, (agents, count) in enumerate(top_sequences, 1):
            output.append(f"  {i}. {' ‚Üí '.join(agents)} ({count} times)")

    output.append("")

    # Recommendations
    output.append(color("üí° Recommendations", Colors.BOLD + Colors.YELLOW))
    output.append(color("-" * 60, Colors.YELLOW))

    if recommendations:
        for i, rec in enumerate(recommendations[:5], 1):
            priority_color = Colors.RED if rec['priority'] == 'high' else Colors.YELLOW
            output.append(f"{i}. {color(rec['type'], priority_color)}: {rec['message']}")
    else:
        output.append("No specific recommendations at this time.")

    output.append("")

    # Time Savings Analysis
    output.append(color("‚è±Ô∏è  Time Savings Analysis", Colors.BOLD + Colors.GREEN))
    output.append(color("-" * 60, Colors.GREEN))
    output.append(f"Total automated time: {time_savings['total_automated_hours']:.1f} hours")
    output.append(f"Manual baseline: {time_savings['manual_baseline_hours']:.1f} hours")
    output.append(f"Time saved: {time_savings['time_saved_hours']:.1f} hours ({time_savings['time_saved_percent']:.0f}% reduction)")
    output.append(f"ROI: {color(time_savings['roi'], Colors.GREEN)}")
    output.append("")

    if time_savings['agent_efficiency']:
        output.append("Agent Efficiency:")
        sorted_efficiency = sorted(
            time_savings['agent_efficiency'].items(),
            key=lambda x: x[1]['time_saved_percent'],
            reverse=True
        )[:5]

        for agent, eff in sorted_efficiency:
            percent_faster = f"{eff['time_saved_percent']:.0f}% faster"
            output.append(
                f"  ‚Ä¢ {agent}: {eff['avg_duration_min']:.0f} min/task "
                f"(baseline: {eff['manual_baseline_min']:.0f} min) ‚Üí "
                f"{color(percent_faster, Colors.GREEN)}"
            )

    output.append("")

    # Quality Trends
    output.append(color("üìà Quality Trends", Colors.BOLD + Colors.CYAN))
    output.append(color("-" * 60, Colors.CYAN))

    complexity = quality['complexity_score']
    status = color("‚úÖ Good", Colors.GREEN) if complexity['current'] < complexity['target'] else color("‚ö†Ô∏è  High", Colors.YELLOW)
    output.append(f"Complexity Score: {complexity['current']}/10 (target: <{complexity['target']}) {status}")
    output.append(f"  Trend: ‚Üì {abs(complexity['change_percent'])}% vs last month ({complexity['trend']})")
    output.append("")

    security = quality['security']
    remediation_status = color("‚úÖ", Colors.GREEN) if security['remediation_rate'] >= 90 else color("‚ö†Ô∏è", Colors.YELLOW)
    output.append(f"Security Issues: {security['issues_found']} found, {security['issues_fixed']} fixed ({security['remediation_rate']:.0f}% remediation) {remediation_status}")

    test_cov = quality['test_coverage']
    output.append(f"Test Coverage: {test_cov['avg_coverage']:.1f}% average {color('‚úÖ', Colors.GREEN)}")

    reviews = quality['code_reviews']
    output.append(f"Code Review Findings: {reviews['issues_found']} issues ({reviews['issues_fixed']} fixed, {reviews['in_progress']} in progress)")
    output.append("")

    # Agent Performance Comparison
    if comparisons:
        output.append(color("üèÜ Agent Performance Comparison", Colors.BOLD + Colors.BLUE))
        output.append(color("-" * 60, Colors.BLUE))

        for agent_type, agents in list(comparisons.items())[:3]:
            output.append(f"{agent_type.title()} Tasks:")
            sorted_agents = sorted(agents.items(), key=lambda x: x[1]['success_rate'], reverse=True)

            for agent, stats in sorted_agents[:3]:
                output.append(
                    f"  ‚Ä¢ {agent}: {stats['success_rate']:.0f}% success, "
                    f"{stats['avg_duration_min']:.0f} min avg ({stats['count']} tasks)"
                )

            # Recommendation
            best = sorted_agents[0]
            output.append(f"  ‚Üí Recommendation: Use {best[0]} for {agent_type} tasks")
            output.append("")

    # Workflow Optimization
    bottlenecks = [opt for opt in optimizations if opt['type'] == 'BOTTLENECK']
    parallel_opps = [opt for opt in optimizations if opt['type'] == 'PARALLEL_OPPORTUNITY']

    if optimizations:
        output.append(color("üîß Workflow Optimization", Colors.BOLD + Colors.YELLOW))
        output.append(color("-" * 60, Colors.YELLOW))

        if bottlenecks:
            output.append("Bottleneck Identified:")
            for opt in bottlenecks[:2]:
                output.append(f"  ‚Ä¢ {opt['agent']}: {opt['issue']}")
                output.append(f"    Suggestion: {opt['suggestion']}")
            output.append("")

        if parallel_opps:
            output.append("Parallel Execution Opportunity:")
            opp = parallel_opps[0]
            output.append(f"  ‚Ä¢ {' and '.join(opp['agents'])} can run in parallel")
            output.append(f"    Estimated time savings: {opp['estimated_savings_min']:.0f} minutes per workflow")
            output.append("")

    # Action Items
    output.append(color("üéØ Action Items", Colors.BOLD + Colors.RED))
    output.append(color("-" * 60, Colors.RED))

    action_items = []

    # Generate action items from recommendations
    for rec in recommendations[:3]:
        if rec['type'] == 'CREATE_WORKFLOW':
            action_items.append("Create workflow pattern for frequently used agent combinations (high priority)")
        elif rec['type'] == 'INVESTIGATE' and 'success rate' in rec['message']:
            action_items.append(f"Investigate agent performance issues")
        elif rec['type'] == 'IMPROVE_SPECS':
            action_items.append("Add dependency research to spec template")

    # Add optimization items
    if parallel_opps:
        action_items.append("Consider parallel execution for test + docs")

    if bottlenecks:
        action_items.append(f"Review {bottlenecks[0]['agent']} configuration for large codebases")

    if not action_items:
        action_items.append("No critical action items - system performing well")

    for i, item in enumerate(action_items[:5], 1):
        output.append(f"{i}. {item}")

    output.append("")

    return "\n".join(output)


def export_json(analyzer: TelemetryAnalyzer, patterns: Dict[str, Any],
                recommendations: List[Dict], time_savings: Dict[str, Any],
                quality: Dict[str, Any], comparisons: Dict[str, Any],
                optimizations: List[Dict]) -> str:
    """Export analysis as JSON."""
    return json.dumps({
        'analysis_period_days': analyzer.days,
        'total_invocations': len(analyzer.invocations),
        'patterns': {
            'top_combinations': [
                {'agents': list(combo), 'count': count}
                for combo, count in patterns['top_combinations']
            ],
            'hourly_usage': patterns['hourly_usage'],
            'agent_success_rates': patterns['agent_success_rates']
        },
        'recommendations': recommendations,
        'time_savings': time_savings,
        'quality_trends': quality,
        'agent_performance': comparisons,
        'workflow_optimizations': optimizations,
        'timestamp': datetime.now().isoformat()
    }, indent=2)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='OaK Analytics Dashboard - Pattern recognition and recommendations',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='Number of days to analyze (default: 30)'
    )

    parser.add_argument(
        '--telemetry-dir',
        type=Path,
        default=Path(__file__).parent.parent / 'telemetry',
        help='Telemetry directory path (default: ../telemetry)'
    )

    parser.add_argument(
        '--json',
        action='store_true',
        help='Output as JSON instead of formatted text'
    )

    parser.add_argument(
        '--no-color',
        action='store_true',
        help='Disable colored output'
    )

    # Section toggles
    parser.add_argument('--patterns', action='store_true', help='Show only pattern recognition')
    parser.add_argument('--recommendations', action='store_true', help='Show only recommendations')
    parser.add_argument('--time-savings', action='store_true', help='Show only time savings')
    parser.add_argument('--quality', action='store_true', help='Show only quality trends')
    parser.add_argument('--performance', action='store_true', help='Show only agent performance')
    parser.add_argument('--optimization', action='store_true', help='Show only workflow optimization')

    args = parser.parse_args()

    # Initialize analyzer
    analyzer = TelemetryAnalyzer(args.telemetry_dir, args.days)

    try:
        analyzer.load_data()
    except Exception as e:
        print(f"{Colors.RED}Error loading telemetry data: {e}{Colors.ENDC}")
        sys.exit(1)

    if len(analyzer.invocations) == 0:
        print(f"{Colors.YELLOW}No invocations found in the last {args.days} days.{Colors.ENDC}")
        sys.exit(0)

    # Run analysis
    patterns = analyzer.analyze_patterns()
    recommendations = analyzer.generate_recommendations(patterns)
    time_savings = analyzer.calculate_time_savings(patterns)
    quality = analyzer.analyze_quality_trends()
    comparisons = analyzer.compare_agent_performance(patterns)
    optimizations = analyzer.identify_workflow_optimizations()

    # Output results
    if args.json:
        print(export_json(analyzer, patterns, recommendations, time_savings,
                         quality, comparisons, optimizations))
    else:
        output = format_output(analyzer, patterns, recommendations, time_savings,
                              quality, comparisons, optimizations,
                              use_color=not args.no_color)
        print(output)


if __name__ == '__main__':
    main()
