### This document is a transcription of Rich Sutton's OaK Architecture: A Vision of SuperIntelligence from Experience at the Reinforcement Learning Conference 2025 ###

### This is the YouTube video linke where the transcription was sourced: https://www.youtube.com/watch?v=gEbbGyNkR2U ###

###Transcription Start###
[Applause] I had I had so much fun talking with
0:05
with so many of you here today. Um and you know we need a conference on
0:11
reinforcement. It's it's clear. I wasn't sure but now I think it's clear. This was the right move. Um
0:21
so I have prepared for you a talk uh on the oak architecture. It's a vision of
0:27
super intelligence from experience. It really is my attempt to um
0:35
address the issues at the center of of AI. And I want to start just by
0:42
recognizing how um how difficult and important is the task
0:51
of AI. Um so
0:56
AI AI is a grand quest. Uh we're trying to understand how people work. We're
1:02
trying to make people. We're trying to make ourselves powerful. Um and this is a profound intellectual
1:09
milestone. It's going to change everything. You know this, but it's good to take a moment and pause and recognize what
1:17
we're doing is incredibly hard, incredibly important. as an intellectual milestone. I think it'll be comparable
1:23
to the origin of life on the earth at least when we under when we when some
1:29
part of the of the of the planet understands how it works and how it
1:34
thinks and how it uh can be so uh transformative to our to the to the
1:42
to the planet. Okay. But it's also a continuation of things that what we've
1:49
always done and it's just the next big step. So now myself I um I think this is
1:58
just going to be good. Lots of people are worried about it. I think it's it's going to be good. It's an unalloyed good
2:04
and I think the greatest advances are still ahead of us. It's a marathon.
2:10
I think you know good for this group that the path to full AI strong AI runs
2:16
through reinforcement learning and not I think through things like uh to non-experential things like large
2:22
language models. The biggest bottleneck is strangely is we have inadequate learning algorithms.
2:29
You may think we have our deep learning and we that's the one thing we know but I think it's not like that at all. I
2:34
think it's it's more like we they are very our algorithms are very crude. they need to be they need to be better and
2:41
that is what we should be working on. Okay. So, and then myself
2:47
uh I've tried to think deeply about this intelligence for half a century. Every
2:52
day I'm sort of in the trenches designing algorithms, trying to design algorithms, seeking better algorithms
2:57
for reinforcement learning, for learning from experience. And I follow this Alberta plan for AI research which you
3:02
may know about. Mike and Patrick and I did it a couple years ago. And today I'm going to talk about the vision of an
3:09
overall agent architecture AI agent architecture called oak and I think it provides a line of sight towards towards
3:17
our grand prize of understanding mind. Okay. So those are my my introductory
3:25
comments. Now let's talk about oak. I think it's fun just to start with the name so you which may be a mystery oak.
3:32
It comes from the idea of options and knowledge. Now, as many of you are very
3:37
familiar with, an option is a pair. Well, actually, you may think it's a triple. I sort of have dropped the
3:43
initiation set in in in my for many last couple decades. Um, so for me, it's a
3:49
pair of a policy, a way of behaving, and a way of deciding to stop behaving in
3:55
the Okay, so just those two, just a pair. And in oak the agent has lots of
4:01
options and it's going to learn its knowledge will be about what happens when you follow the option. So in this
4:10
way the agent is meant to learn a high level transition model of the world that enables planning with larger jumps and
4:16
hopefully carves the world at its joints. Um yeah so that's that's uh where the
4:23
name is from. I think it's is a grand challenge and a grand quest. And so I
4:30
show it like this that we are seeking the holy grail. The holy grail of AI.
4:36
Let me put that up in a way it's easier to read. Uh we want this
4:43
we want this AI design that is domain general contains I'm going to say
4:50
nothing specific to the world. We want a general idea. So I'll talk about that
4:55
more in a minute. But let me just put down my three main uh design goals. It
5:01
should be domain general. It should be experential. That is the mind should grow from runtime experience not from a
5:09
special training phase. And third it should be open-ended in its sophistication in its abstractions.
5:16
So that um it's a it can it can form any any concepts in its mind that are needed
5:23
to deal with whatever world it's connected to limited only by its computational resources. So those are
5:29
the three main deterata and we'll talk about them. I guess first
5:35
I want to establish some words. I want to talk about I'm going to talk about design time and runtime. Design time and
5:42
runtime. when you're in the factory being designed then your robot goes out and lives its world that's its runtime
5:48
sort of a lot a lot the way Leslie spoke um so the age is designed s at at design
5:56
time we would be building in any domain knowledge that we might have
6:08
farther away from the chest might be
6:13
just pardon me.
6:19
Okay, cool. At design time is when you're building
6:24
in any of your domain knowledge and then at runtime is when you're actually interacting with the world. Learning
6:30
from experience, making plans that are specific to the part of the world you're in. So like uh I don't know a large
6:37
language model everything is done as design time and when it goes out to be
6:42
used in the world it doesn't do anything. My emphasis is going to be the other way around. We want I want to do
6:47
all the important things at runtime online on the job.
6:55
And in a minute I'll be talking about a big world but a big the idea of a big world a big complex world is that you're
7:02
you're not going to be able to build things in it. You're not going to be have your agent know everything about
7:07
the world in the factory because the world is huge and you can't know everything. Your a your robot is small
7:14
compared to the world. And uh so if you want to be able to learn arbitrary
7:19
open-ended abstractions um you need the world to find out whether the right abstractions for for the part of the
7:25
world you're uh running into. So you got to do it at runtime. So I don't know. So
7:32
I'm going to talk about this a lot. runtime. Everything has to be done at runtime and why that's actually a good
7:39
way to think about it. Okay, so let's just ask the question. Yeah, should an
7:44
agent's design this is a question for you guys, right?
7:49
Should the agents design reflect the world in which it's expected to be used?
8:00
Good thing about this question is both answers are
8:05
wrong. I mean both answers are right. So you
8:11
can um yeah if you want something to uh perform well and you want to put it out
8:17
there and have it do something good right away, you know, you want to you want to put design domain knowledge in
8:22
there. You want it to reflect the world. But if you want a good design, so I'm
8:27
gonna say no. My quest, my quest is that the design should not depend on the
8:33
world at all. Okay. Uh it should be domain general. Now
8:41
this is really just you know there there are multiple questions and there are multiple uses and like you know I I I
8:48
have to respect totally you know someone who wants to do an application make something that's useful make something
8:54
that performs well sure that's important but also important is let's understand
9:00
the mind in a simple way let's let's what we what we would want is a a
9:08
conceptually simple understanding of what's going on inside a mind. That's
9:14
that is in some sense the grand quest of of AI is to understand what it means to
9:20
achieve goals, what it means to understand an arbitrary world. It's it should be simple and if you the world
9:28
the actual domain that you're going to interact with is arbitrarily complex and so your goal your job your agent's job
9:35
is to go out there and learn all the fiddly wonky little details and highle
9:40
structures of the world that it's going to encounter. But you really don't want to understand what it's doing at that in
9:47
terms of all those domain specific details. You want a level of understanding that is at a higher level
9:53
and is based on principles and not the intricate complex details that that are
10:00
in the world. So it's sort of different purposes. Someone who wants to go have something
10:05
that will perform really well right away or someone who wants to have a conceptual understanding of what a mind
10:11
is and how intelligence works. So my quest is to do the latter. And so it's only natural that uh that we would want
10:20
to exclude those things. So we of course I have to reference the bitter lesson at
10:25
this point. Um and I'll just read this. The actual contents of minds
10:32
are part of the arbitrary intrinsically complex outside world.
10:38
They are not what should be built in as their complexity is endless genuinely endless. our our agent has a
10:46
big computer and we don't want to understand everything going on in there. Instead, we should build in only the
10:53
meta methods that can find and capture this arbitrary complexity.
10:59
In short, we want agents that can discover like we can, not which contain what we have already discovered.
11:07
So that's the idea for the purpose here in a scientific conference trying to understand what a mind is, how it should
11:15
work. We want we want a level of description that is above all those domain specific uh endless details.
11:25
Okay. Now second question, should the agent learn from special training data
11:32
or should it learn only from runtime experience?
11:41
only. You must be triggering a little bit on the word only.
11:46
Only from runtime experience.
11:52
Well, for me the agent should learn only from runtime experience
11:58
should be entirely experential. And the reason is again um
12:05
we want a conceptually simple design and if it's possible uh to learn only at
12:12
runtime that would be a simpler a simpler understanding and um
12:18
so we should seek this. So the form of the argument is going to be there's certain things that that have to be done
12:25
at runtime and and but and could be done at design time and uh so basically
12:31
everything has to be done at runtime maybe also at design time but it ha at
12:37
at runtime it ha you have to be able to learn you have to be able to change your abstractions you have to um be able to
12:44
make a model of the world you have to be able to plan with that model every all those things have to be done at runtime
12:51
because you're going to encounter the world. The world might not be like what you expected and certainly you won't
12:56
know all the intricate details and all the abstractions that are needed for for the part of the world that you're interacting with. So all those things
13:02
have to be done at runtime. Now you could also do some of them at design time but it's sort of in some sense that
13:08
would be optional. You know, that might speed you up, but since they all have to you have to be capable of being done at
13:14
at runtime, why not make a conceptually simple design and just doesn't worry about trying to get a head start on the
13:22
problem, but just has the uh the u runtime aspects. And so that's what
13:28
that's what my quest is. That's the that's the the uh journey I'm the quest I'm setting out on. And I'm hoping
13:34
you're going to join me. Okay. So I've talked about the big world perspective.
13:40
So let's let's do that. Let's talk let's make let's gain a common knowledge amongst us
13:48
of what that means. The big world perspective or the big world hypothesis something that's been floating around at
13:54
least in Alberta for like five years and we've all come comfortable with it. Um and it's really influences all of our
14:00
thoughts and our designs. So the idea is simply that the world is bigger, more
14:06
complex than the agent. Uh and it's much bigger really. It's bigger than this. It's it's big, you know, it's really
14:13
big. And it's got to be much much bigger than the agent because the world contains,
14:18
you know, billions of other agents and um and all of course all the atoms and
14:25
all the intricacies of uh the objects spread around. um the world is much more
14:30
big and and the world contains what what is happening in all those other agents
14:35
matters to you. It matters to you what's going on in the minds of your your friends and your loved ones and your
14:41
enemies. Um all all those things are important to you and they have to be taken into
14:46
consideration. And so you know the upshot is that nothing you the agent is
14:52
going to be doing is going to be exact and it's so and it's not going to be optimal. It's going to be approximate
14:59
when you make a value function. It's going to be of course be an approximate value function. Uh and your policy will be will not be the optimal policy. Your
15:06
transition models will be much enormous enormous reductions. They're sitting
15:12
inside your head. You know, your model of the world and the world is out there much much bigger, right? Um
15:21
even a single state of the world, you can never hold it in your head. You can never hold in your head all the the
15:29
states in everyone else's minds. So, one of the most important
15:35
consequences of this is that the world ends up appearing non-stationary.
15:41
Uh, and I'm citing a little paper on this that Dave Silver and and a coupe
15:48
and I wrote where we just made this point. The world, you can probably see
15:54
it. the if you don't have a model, you don't have a good sense of the state of the world and it's it's big and
16:00
sometimes you're there, sometimes you're there, things look the same, your function approximator cannot capture everything, the world's going to look
16:07
non-stationary and so you have to learn at runtime because
16:14
you're going to you can't you can't have built in everything about the the whole big world at design time. You have to
16:21
learn at runtime. you're going to encounter some particular part of the of the world at runtime and you want to uh
16:27
customize and be appropriate for that part. You know, you go to you go to work, you're you're you're AI agent
16:34
that's supposed to be play a productive role in society and it goes to work and it meets its co-workers. It has to
16:39
remember the name of the guy it's working with. You know, that was not in the domain knowledge. It has to remember uh the work that they've done on that
16:46
project. What's working well? What's not working well? What what are they trying to do? All those you know everything you
16:51
do in your life uh is is is un could not have been foreseen
16:58
and to to um so I know it should be obvious you have
17:04
to learn during your life. You have to plan during your life. I'm
17:11
Leslie Caling made this point that planning is required because the world is big.
17:17
Um, and so this also applies to your abstractions. You know, my third
17:22
desitter is that we want an open-ended abstraction. We want to be get more and more sophisticated understanding of this
17:29
particular world. We have to find, you know, what's the right joints and ideas that are involved
17:36
in the world that we're encountering. And so, um, yeah, you you may be able to also
17:44
add abstractions. Maybe you believe in objects like Leslie and and you want to build that in, but
17:51
that doesn't get you out of having to to have the ability to create new abstractions at runtime. And so, if
17:58
you're going to have to create them at runtime, you know, why don't we just do it in one place and and and
18:06
uh that'll be a very good start. You know, I think of a design for an AI, the
18:11
perfect design. It would not be a huge thing. It would not be like an encyclopedia or a library worth of
18:18
knowledge. It would be like uh well, when I make for me actually it
18:25
almost fits on a slide. You write in the pseudo code, you know, maybe it's three slides. Okay. I thought I think
18:32
something of that that order. five pages for your uh description of
18:39
all the essential elements that are domain independent and and and and yet
18:45
are capable of arbitrary um open-ended abstractions.
18:50
Okay. Um yeah, so this talk like hey um
19:00
I was up till like the like four o'clock last night making making these slides.
19:07
Um this is this is um this is sort of a new talk for me. This
19:12
is the first time you're the you're the first guys hearing it. I've been I've been traveling around the world and giving all these uh philosophical talks
19:19
and political talks and point of view talks and that's great and I've enjoyed that but but uh you know I really
19:26
thought here I'm I'm at the RLC conference reinforcement learning I should do something uh substantive
19:34
maybe even technical um so so I um
19:40
and then all during the week you know I went to every I went to all these talks I talked to all these people And uh I I
19:46
keep just changing my mind about what I want to say. And uh so anyway, this is all my way of of excusing myself or
19:53
explaining to you that that this talk is new and it's not quite polished. In in
19:59
particular, I might say some things more than once. Um so and maybe it's okay to
20:06
have some repetition if they're important things. Um okay, but just be aware of that. and maybe maybe kick me
20:12
if I if I uh need to move on to the next thing. Okay, I'll try to be quick.
20:18
Runtime learning I think always wins over design time because the the world is much bigger than the agent, the big
20:25
world perspective. Um design time can't cover every case. Runtime learning can customize to the
20:31
part of the world actually encountered. Runtime learning scales with available
20:36
compute whereas design time learning or anything done at design time scales with the
20:42
available human expertise at design time. It was the only thing available
20:48
and historically scaling with compute wins in the long run. That's what the bitter lesson is is explicitly about.
20:55
However, today's deep learning methods, runtime deep learning methods, continual
21:01
learning, they don't work very well. Okay, this is this is a a a big bitter thing
21:08
for me. I wish they could worked well because I'm talking all about runtime learning and I want to use it.
21:15
Um yeah, if we if we one last thing about runtime
21:23
learning, it does enable metalarning. Metal metal learning is where you like try learning one way and then you try
21:29
learning another way and you notice that oh this way works better in the future I will do this. If you were if you were
21:36
doing everything in one shot you couldn't do that. This idea of becoming
21:41
better at learning requires uh one time you're doing learning another time
21:46
you're trying a different way of learning and you pick the better one. So metalarning really requires this to be
21:52
done at the at the runtime. Okay. Now
21:59
let's think about the problem just a little bit more. You know I like to separate things into the problem and the solution. Really almost everything I've
22:06
been talking to you about is the the problem the quest. What are our goals? What are our desitter?
22:13
So um one more slide on that. The AI problem is to design an effective
22:20
purposeful agent that acts in the world. And the classic reinforcement problem is
22:26
the same thing except we add the purpose is specified by scalar reward signal.
22:32
The reward and the world is general and incompletely known. But the world can be
22:38
anything could be grid world or the human world can be stochastic, complex, nonlinear, non-marov.
22:44
The state space of the big world is effectively infinite and its dynamics
22:50
are effectively non-stationary. Um let me go ahead to just talk about
22:58
this one a little bit further. The purpose is specified by a scalar signal.
23:04
So that's we have a name for this idea. It's called the reward hypothesis. And I
23:09
I wanted to bring this up because we have thought about it and uh it's not like a quick choice without intention.
23:17
Um so the reward hypothesis is this that all of what we mean by
23:23
goals and purposes can be well thought of as the maximization of the expected
23:28
value of the cumulative sum of a received scalar signal called reward.
23:35
Um, so there's lots of specific things there like the expectation, like the cumulative sum. Um, yeah, and that's
23:44
been thought through. And the SC the idea of a scaler reward, I just want to say it's not just a uh something we
23:50
haven't thought about. In fact, it's a it's a great thing. It's a really clear way to specify the goal. It's become
23:57
popular in many different disciplines, not just AI, but also economics and psychology, control theory, and forever
24:05
people have been trying to modify. They've been trying to add things like constraints, multiple objectives, risk
24:10
sensitivity, and and and I I I
24:18
don't know. I I hate that. I mean, even if it was good to do, I I I
24:25
I don't want it to be done. I don't want it to be true. I I you I think you've already gotten the sense. I like things
24:32
to be simple. You know, that's like a really high high uh uh deserata
24:39
desire. I want things to be simple and I might even simplify them a little bit too far in order to uh to uh be clear
24:48
and uh so anyway, I want things to be simple. Do we need to do we need all these things to get generality? That's
24:54
the real question. and Michael Bowling and others have have written this really
24:59
nice paper called settling the reward hypothesis where they go through all these cases and and I don't know if I
25:07
want to uh they they they establish that in a
25:12
certain sense the reward hypothesis is correct that that you don't um add
25:17
generality um uh by adding multiple objectives or risk sensitivity or any of
25:24
these constraints. So they it's one way of validating that choice and you might also probably know
25:30
the reward is enough paper where we argue that even a simple reward can lead to all the attributes of intelligence in
25:36
a sufficiently complex world. Okay. So
25:41
um now I want to talk about the solution methods the architectures
25:48
obvious starting place is model free reinforcement learning basic reinforcement learning where the agent
25:54
constructs a policy and value function at runtime
25:59
both these are functions all all these are runtime RL architectures okay the
26:05
model free and then you can handle the non-markoff case if you uh construct
26:10
your feature uh state representation from u uh from from your data and I'll
26:18
show you the the picture of that in a second. Uh but still better uh would be to make a model of the world and use
26:25
that model to plan with potentially better. Now the oak architecture is
26:30
along the same line um of improvement of of extension and the the thing about the
26:37
oak architecture is it it adds to those things u auxiliary
26:42
problems subpros and those sub problems are in the form of attaining individual
26:47
features individual state features and in this way uh we enable the discovery
26:54
of higher and higher uh levels of abstraction and we we achieve this open-ended uh
27:00
goal. Okay. So, as a in a picture, this this picture on the on the left is from
27:07
uh the textbook, the reinforcement learning textbook. Um and this is the very last figure in the book. And so
27:15
maybe it's familiar to some of you. Uh we have uh the world and then the agent
27:21
is everything above the world. It's the policy and value functions. It's a model of the world. It's a planner. And it's
27:26
the Ubox. The viewbox. I want you to notice because this is a is a um kind of
27:33
reinforcement learning where we don't assume that the state is available to the agent. We only observations are
27:39
available to the agent. This we send actions to the world. The world sends back observations and and a reward.
27:46
Okay. And then there's a a process here that that's a part of the agent uh that
27:52
that computes uh something that we'll use as a state representation uh by the
27:57
by the policy and value function. Okay. So that's the construction process and
28:03
um nowadays I draw um things uh a bit
28:09
differently. Um and this the right figure is the full oak architecture and
28:15
you see the many of the same components. This U box is now called perception. Perception is a better name for it
28:21
because what does it do? This perception process it takes in the uh the data
28:27
that's happening the actions and the observations and it forms a a sense of where the agent is now. That's really
28:34
what perception is about. take in your sensory input, get a sense of where you are now. Use that sense to make your
28:39
decisions to to as input to your policies, your value functions and your models.
28:45
Okay, so the oak architecture has all those things, but it also adds auxiliary subpros and those each subpro will have
28:53
its own value function and its own policy. Uh so that's what's suggested by
28:59
having uh these u shadow policies behind the the main policy and these secondary
29:06
auxiliary value functions behind the main value function. Um
29:13
also each one of these subpros is going to be based on a different component of the state feature representation. So
29:20
this is the thing that acts like state and I want you to think of it as a feature vector and each one of these sub
29:26
problems is going to be based on a different component of that feature vector. So that's what it looks like as
29:32
a picture. Okay. Uh now we're going to get into the a bit of the nitty-gritty.
29:39
We're going to look at it. I've given all these these quick introductions to the idea tell you I've told you various
29:45
properties of the oak architecture. Now I want to tell you exactly what it is or
29:50
it is these these what eight steps done in parallel at runtime.
29:58
Okay, it's a lot of steps. Um we're I'm going to come back to this slide a bunch of times uh and develop each part of it.
30:05
So just relax and let's get started. Let's just learn what some of the the the characters are here and how they
30:13
interrelate. So we might start with the first line. Um and this line uh is
30:20
learning the policy and the value function for maximizing rewards. That's like normal reinforcement learning. And
30:27
I think that is is is almost done. If it was done, there would be a green check
30:33
mark, but it's blue. And blue means it would be done if we could do this
30:38
continual deep reinforcement deep learning thing with metalarning. You know if we really could do continual
30:44
learning that would be uh that would be done. And so we can do this with in very
30:49
simple simplified cases. We can deal it with uh some of the algorithms like continual backdrop. We can deal with um
30:57
the linear case. So this is gets a blue check mark conceptually
31:04
done but um really it's waiting to be done well. it's waiting to uh solve this
31:11
problem of continual learning for deep learning.
31:16
Um now the next one uh is red because we don't really have a solution. We have
31:22
lots of ideas but we don't have a specific proposal and so I'm going to come back to this later. The second one
31:27
is generating new state features from the existing features. And let me go
31:33
over this next bit uh quicker. Let me just run all the way down verbally through all eight steps. We we we're
31:40
going to have some features. We're going to order the features. We're going to take the highest ranked features, the most important features according to our
31:47
estimation, and we're going to create subpros of of achieving them. So if I decide that being in this lecture room
31:55
is a is an important sub goal, I would make a a sub problem for that would be rewarded when I when I when I not would
32:02
would be would would succeed when I am here. If I think uh holding this microphone up sufficiently close to my
32:08
mouth is a good sub goal. Um, I would I would uh I would I would make that
32:14
feature into a a good subpro and so on for finding the restroom and finding the
32:21
the the uh the coffee. Coffee is a really good you know feature for that
32:27
flowing into your mouth and getting all the sensations involved in that. So that feature becomes a subpro for attaining
32:34
it and then you learn solutions. So this is the heart of of um the heart of the
32:43
oak architecture is to have sub problems. You learn the solutions of the sub problems. The sub problems are the options from the from from the O and
32:51
oak. And uh we also have to learn the value functions that are associated with
32:56
the sub problem. Okay. And then we're going to have these options. And the
33:02
next step is we're going to have we're going to learn models of the option. We want to know uh what will happen if you
33:09
if you were to execute any any one of them. This will be part of your model of
33:15
the world but it will be a high level model of the world because it'll be about uh an extended way of behaving
33:20
rather than about a single action. These models will enable you to plan and then
33:26
and then you so those are all the the full main steps. Okay. So you've seen it
33:33
once. uh you're going to have to maintain metadata on on the utility of everything and and curate uh throw some
33:41
things out um and propose new ones. Okay. So now we're going to go through
33:46
these steps and for a while I'm going to spend a lot of time um actually on the
33:51
fourth one but yeah the the ordering the features it seems kind of easy but but
33:57
we can't do it until we have all the rest all the other pieces done that a couple cases that will h that will
34:03
happen. Okay let's spend some time about the creation of the subpros one for each
34:10
highly ranked feature. Okay. So on acknowledge there's a long
34:16
history of looking at subpros that are distinct from the main problem. People talk about curiosity, intrinsic
34:22
motivation, auxiliary tasks. Some things are settled, some things are unsettled.
34:28
You don't have to read all this. I want to direct your question to the red part. The key open questions about subpros,
34:35
which are what should the subpros be? Where do they come from? How can the agent or can the agent generate its own
34:42
subpros and how do the sub problems help on the main problem? So the contribution
34:49
of oak is to an is to propose answers to all these questions and to really uh
34:54
answer questions like uh the third one how can the agent make its own subpros in the affirmative and thus get
35:01
open-ended abstraction. Um so I like to think of it very
35:06
basically that we have problems and solutions and these interact with each
35:12
other. We propose a problem to work on. We work on it. We solve it. As as a part
35:17
of solving it we will make new features and those features will then be the basis
35:23
for new sub problems and and and then the sub problems will have to be solved
35:29
new features and so on in an endless cycle. roughly. That's what I'm talking about. And I wanted to give some
35:35
examples from nature. Uh here's an orang ba a young orangutang um playing swinging. And so I What is he
35:44
doing? Like he's not getting food. He's just interested in what it feels like
35:50
when he swings. Yeah. Okay. That's what I think he's
35:57
doing. I think it's that sensation is interesting. and and he got it once and now he's trying to get it again and
36:03
understand how to control it. Um yeah, so here's and also on the other
36:09
slide we have an orca uh who
36:19
somebody threw this big uh I don't know what to call it right now.
36:24
A buoy into his into his pen and he's decided trying to figure out what he could do with it. and he's managed to
36:30
get it up on his back. So, that was not not random. Um, he got this idea and now
36:36
he's perfecting it. Yeah. So,
36:41
animals play, people's play, uh, infants play, young people play. Um,
36:49
so this is a sped up video of a infant playing. And, uh,
36:56
this is what we want. We want the the way the the child goes from object to
37:01
object, learns a little bit about it, gets bored, moves on to the the next object, and just gradually develops a
37:08
better and better understanding. Maybe next time when he comes back to the to an object, he'll he'll he'll uh have an
37:14
increased ability, be able to do new things with it. Um, this is what we want. And so I'm trying to think about
37:21
them as posing sub problems for themselves, things to learn about, things to understand, things to predict,
37:27
and uh things to control and and figure out where it can make progress in in
37:33
learning solving the sub problems. Okay, so maybe you're ready to accept this
37:39
statement. The agent must create its own sub problems. Sub problems can't be given to you. Can't be given to you at
37:45
design time. You've got to create your own that's far too various and world
37:53
dependent to have been built in. We have to give the responsibility of the the questions the problems not the solutions
38:00
not the features the questions okay what is the pro what is the how can we how
38:08
can we um do this I mean we have much of the machinery the machine of options general value functions off policy
38:15
learning planning methods these are machinery to help us in this process but
38:21
we want to create them in a domain independent way and that's challenging Okay. So I want to offer this this
38:28
possible way to make subpros in a totally domain independent way which is
38:34
um when you come across a feature when you make up a new feature or you experience a new feature you can make it
38:40
to be uh the basis of a sub problem uh I call it a reward respecting subpros of
38:46
feature attainment. So let me show you exactly what that is.
38:52
um how do we create a subpro from a feature? So a feature is like yeah
38:57
feature eye. It's it's a bright light that you saw once. It's a it's an
39:02
interesting sound that happened. It's you you're a baby and you heard the rattle make a sound. You'd like to reproduce that sound. So you have a
39:09
feature I a feature index I and you have kappa which is how intensely you want
39:16
that feature. you have to express that uh and you'll get different um subpros
39:21
if you if you want it at all costs or if you just kind of want it a little bit.
39:27
So the sub problem is to drive the world to a state where the feature is high
39:33
without losing too much reward because you will lose some reward if you're not doing what you normally do because what
39:38
you normally do is uh maximize reward. There is just one reward by the way and
39:44
so I don't have to qualify that is the real reward and the sub problem is to
39:50
achieve a state where the feature is high uh without losing too much reward
39:56
without having to go through something that's painful or having lost opportunities to get something uh
40:04
pleasurable. Okay, so we're trying to find an option. An option is a pair. It's a policy pi
40:10
termination function gamma that maximizes the value of the i feature at termination
40:16
while respecting the rewards and values. So here's the equation. Maybe we can understand the equation. Uh in each
40:23
state you're trying to choose you're trying to maximize and you're going to choose pi and gamma to maximize. And
40:28
it's the sum the sum is conditional on starting uh the world in in the indicated state
40:36
because um yeah for each for each state we say if
40:42
we started there have a policy pi that gets you rewards
40:48
um from t+1 to t. T is is the time of
40:55
termination. You're going to follow the option pi and you're going to terminate when gamma says terminate. And so that
41:02
will fix establish the random variable which is the time which you terminate
41:08
capital t. And if you look at all the rewards you receive while you are following the option summing them up.
41:13
Those are the things that you want to be as big as possible or not as least negative as possible.
41:21
And then you want to um reward yourself for achieving feature I's feature I at
41:29
time of termination S sub capital T. And you know there's a there's a waiting by
41:35
kappa. So you want lots of rewards. You want to be the feature to be true but
41:43
you know it's got to be traded off the rewards. And you also care about the state that
41:50
you're in at the time of termination. You don't want to like find a really good way to to uh I don't know
41:57
get some coffee but has the consequence that you have to break your leg. Okay.
42:05
Actually, that would be a reward. That would be a bad reward. You don't want a way to get coffee that would um uh leave
42:11
you in a bad state. Like let's say you got coffee but uh you know you're gonna get arrested or you're going to fall
42:19
down the stairs. Okay. Um those are bad states. You know I often you know the
42:28
walk along the edge of a cliff but uh don't fall off. If you fall off, it actually doesn't hurt very much to fall
42:34
off because, you know, if you can say just terminate while you're in the air, uh the rewards are fine, but the value
42:41
the value is, you know, bad rewards are coming up. So, your value will be will be poor.
42:47
Okay, that's how we create a sub problem. And now let's really we're getting into the heart. Uh we have these
42:54
processes. Um the f the first one is we form the uh the problem just as we just
43:03
talked about you know given a feature form a problem. We do that with all the high highly ranked features. So we have
43:09
now we have you know dozens of pro of problems. Each one we work on it to
43:14
produce an option. The solution to sub problem is an option. Now you've got this options. Well that defines a
43:22
correct transition model for the option. Is it well defined? Um
43:28
what we know what the model should be. If you give me the way of behaving and the way of terminating, we know what the model should be. And so this is
43:35
something you work on. You work on computing this model, approximating that model. Once you have the model and that
43:42
you have a models of all the different uh options for all the different subpros for all the different features and once
43:50
you have the model, you use the model of course to plan and to improve your behavior. Okay. So we got these three
43:55
steps and there's one fourth step which is that uh you have to come up with features right we we we started with a
44:02
good set uh with the highly ranked features so we have to have a way of ranking the features and um and I just
44:10
want to point out that we have that because all of these these these three uh pillars the the later three all use
44:18
features to to do their job right if you're going to find the option the option is a function of state and so you
44:23
have to look at state features when when should you do the option when when it when is the value function of the option
44:30
uh when should it have which values it's going to look at the state features to make those decisions when you uh learn
44:36
the models of the options you're going to look at the state you start in and you're going to look at the state features of that state and you're going
44:42
to say oh that feature I found useful that other feature was useless to me um
44:48
and so and and then when you use the models you will find some models are
44:53
useful And that will sort of trickle back to evaluate the choice of the of the options. And that will also trickle
45:01
back to evaluate the choice of the feature attainment problems. And at least uh all these learning processes,
45:08
predictive learning processes will use the features and they will provide feedback to the uh to the features
45:15
saying these are the ones that have proven useful to us. These ones have not. Okay. So let's draw that the same
45:21
idea with a different picture. I'm going to have many pictures about the same idea and I'm going to say it a few times
45:27
so maybe you'll you'll get it. Um this way of talking and thinking. So the
45:32
perception process is going to is responsible for constructing interesting state features. Um the play process or
45:40
the problem posing process problem posing and solving. That's where you do the sort of core reinforcement learning
45:46
things of figuring out your value functions and your policies and you produce the options.
45:52
And then you have to predict the consequences of those options to form a transition model. And then of course you
45:58
plan with the transition model to get improved policies and values. And the feed we close the the cycle is we have
46:06
feedback from the later steps back to the construction of features. And that feedback is mainly say saying I have I
46:14
have found that feature useful or I have not found that feature useful. Okay. So here's we're back to our eight
46:22
steps. U we now understand what it means to create the subpros one for each feature
46:29
and we also know what it means we've talked about how you learn the solutions
46:35
um and the transition models. Um maybe there's I'll say one more slide about that about this this topic uh how we
46:43
learn those things. Oh, but also notice they're they're in blue because although we know how to do these things, we don't
46:49
really know how to do them with continual deep learning or maybe we have to use Shbanch's continual backdrop. You
46:57
know that this is this is this is a topic we'll come back to. This is incompletely understood. So we kind of know how to do them but we definitely
47:05
think we can do better. Okay, one one short slide
47:11
more about that is it to a large extent we could use just standard off-the-shelf algorithms standard offtheshelf usually
47:17
off policy algorithms for learning general value functions like GTD and emphatic TDD and retrace ABQ uh these
47:26
are prediction learning methods for generic uh GVFs and so we can use that
47:32
to learn the main problem how to get reward we can learn that for learning diagrams for the sub problems. We can
47:38
find the transition models of the options with these methods and the planning can also be done with standard
47:44
algorithms applicable to all GDFs. And this enables us to say that anything
47:49
that can be learned can also be planned. That's I just wanted to get to that slogan for you because it's it's a it's
47:55
a good one. It's a it's a um it's a bit advanced but almost it's
48:01
almost a next step. Okay, so we got those things and now the
48:06
other big step and uh I'm going to have to do it a little bit uh not in full detail, but we
48:14
have to talk about how the planning works. Okay, how does the planning going to work? And I'm going to give that a green check mark because I think we do
48:20
understand this. So planning, why do we plan? Why do we want these jumpy uh
48:25
temporally extended models of the world, the option models? And we we but basically why do we want to plan at all?
48:31
We want to plan because the world changed and the correct values change and it's easier in many cases not in
48:37
every case but in many cases it's easier to get the model of the world right than to get the values right. So you get the model right and then you do the planning
48:44
uh to make the values consistent with your model. Um and uh uh so in this big
48:52
world setting it's it's the world changes or appears to change. Uh most of
48:58
the world's dynamics or in many cases the world's dynamics including the reward parts don't really change but the
49:05
values nevertheless change. Like it's always true that I can walk over there and find the restroom but it's not always true that I want to go to the
49:11
restroom. Um it's not always true that I want to get coffee. it's not always true
49:16
that I want to go to the library all the things or go to Edmonton. So u to
49:21
prepare for these later wants um these these different values you plan and u
49:28
this also has some implications for which sub problem is useful. Okay. Now
49:33
how does planning work? Um I like to think that planning is is by
49:39
approximations to value iteration. So, so this this equation is value
49:45
iteration. You may already know it, but I think maybe you should look first um
49:51
here. What is the model? Model is something that that takes a state a low-level model takes a state and an
49:58
action gives you um a probability distribution over next states and the expected reward along the way. And so
50:06
value iteration then says I'm trying to improve the values of some states. I look at the possible actions and I'm
50:12
going to maximize over them. I'm going to look at the immediate reward and I'm going to discount and then take the
50:18
probability or the really the expected value of of the value of the next state.
50:25
So this is the probability of each next state. You wait by that you take the value of the next state. So you know you
50:30
probably are are familiar value iteration works like that. Um and really
50:35
all all planning methods in some sense work like this just apply to different states in in the search tree and you
50:43
know order which states are updated in such a way uh has is is is
50:49
varied. Okay. And um
50:57
the interesting thing about planning with option models is that it's the same. It's really it's really the same.
51:05
Although life is lived one step at a time, we have to plan it at a higher level. So our knowledge of the world
51:11
should be about the large scale dynamics. It should not be conditional on single actions but on a sustained way
51:18
of acting that is on an option. So if we look at
51:24
the conventional model it's we receives an action an option model you receive an option still you get a probability
51:31
distribution perhaps over the next states and expected reward not not a onestep reward but some reward while
51:38
you're following the option and then then value duration is almost unchanged.
51:43
We just change the actions into options and we still talk about the reward for
51:48
following that option and the probability of each next state under the option. Okay, so that's good. Um that's
51:56
how that gives you a flavor of how planning would be done. You you would you would you you basically you say oh
52:02
here's some state. What are the things I could do there? What's the best I could do? I'll update my value and here's I
52:08
could imagine another state. Maybe it's the state I'm in. Maybe it's not the state I'm in. But I go through this outer loop of of considering various
52:15
states and then maxing over the possibilities and uh doing this equation.
52:22
But I'm sure you are concerned because I've been talking about via s which is
52:27
the value of an individual state and uh we can't do that of course we have to
52:32
have function approximation. So I don't know I don't think it's that
52:38
helpful to go through these equations but yeah the value V of S will become
52:43
the approximate value of a state given a parameter vector W and also your model
52:49
of the world will become R hat and P hat they also become parametric
52:55
and then after you've done that um things are much the same there is more
53:00
complications which I will skip having to do with what's computationally expensive. You can ask me about that if
53:06
you want. Um, if you're interested, I'll say some summary things. Um,
53:13
so, so we we're maybe we're almost done, but
53:19
remember I said we would come back to some things. So, I said we would come back to um the first two learning how
53:28
we're going to learn these things and what's we the problems we have. I want to say explicitly what what the the
53:35
situation is there. So two more slides. Um so the oak architecture requires
53:43
reliable continual learning, continual deep learning. And so this is one of those things that I said this would be
53:49
we'd be all done with step one uh if we uh could do continual learning. And but
53:56
we can't do we can't can we do this yet? Okay. Can we do this? Okay. Do we have
54:02
reliable continual learning? Well, we do have reliable continual learning for the linear case, for the tabular case, but
54:09
for the nonlinear case, for the deep learning case, we can't have reliable we
54:14
don't yet. Do we? No, we sort of do. We have uh we have we have these catastrophic failures
54:22
like u like uh catastrophic forgetting and the catastrophic loss of plasticity
54:29
uh that have been figured out long ago and also very
54:34
recently. Um so we have these catastrophic problems
54:40
but we also looks like there's a range of solution methods. So I guess this is an area that's that's right now in flux
54:47
and uh people are figuring things out and you know we're not there I can't
54:52
give it a green check okay but um but there are lots of ideas continual
54:58
backrop is one of them the metalarning of new features I think can also help
55:03
and related to that it's the other the other problematic uh or the other step
55:09
that I wanted to talk about where we having to do with um creating generating
55:15
new state features. This is this is also a super old problems going back to the 1960s like Minsky and Selfridge would
55:22
talk about this. They would talk about representation learning. They would talk about the new terms problem and anyway I
55:28
like to talk about metalarning nowadays. So backrop back in ' 86 was supposed to
55:34
solve this. We were supposed to you know learning representations by gradient descent. Um but it really just doesn't.
55:41
And um I think we're we we accept we recognize
55:46
that unless we are still in love and think that gradient descent is enough for everything. Um
55:53
most of the other methods other than gradient descent are based on generate and test ideas where you like generate a
55:59
bunch of features and then you test them to see if they're useful and so you could generate them randomly and you
56:04
could test by their utility and u continual backdrop is an instance of
56:10
that. It also has a real old history. Leslie Keelbing did her uh did some of
56:15
this work in her PhD thesis in 1993. Uh Rupam Mahmud and I did some work a
56:22
decade ago. Um there's lots of ideas but not yet a specific proposal for a whole
56:29
network based on grad descent or any other uh there aren't I anyway don't have a
56:36
specific proposal uh for solve this problem. I think it's a really really important problem and I think it's like
56:41
maybe it will be worked out in the next couple of years and then it will literally um take over everything that
56:48
people have done with with deep learning. If we had a deep learning method that was can do everything we're
56:53
doing now but can also learn continually that would just um
56:59
be a really big thing and I think there's no reason why it couldn't happen. Um and so I think it will
57:08
I also think that something like uh my algorithm called IDBID or IDBD and
57:15
it's really old will be a key part of that uh solving this problem.
57:22
Okay. Um
57:28
I'm sort of done. This this figure was just to um remind you one more time of
57:36
the cyclical nature how we have state features that produce sub problems that are solved produce options that are that
57:43
are used to form models and and this is not doesn't look like a cycle but
57:49
remembering that um there's feedback being sent from each user of the features uh information information on
57:57
which features are are useful which ones are that informs the features and so it is in fact a cycle.
58:04
Um so in my the quest uh have we uh succeeded we have something that's
58:10
doesn't it's totally domain general there's nothing in it specific to any world it's totally experential and has
58:17
the claim or the hope that it will be uh you know able to find unlimited open-ended abstractions uh limited only
58:24
by the computational resources and so we might ask you know what do you what what should one think about this
58:31
and so I think arguably reinforce Enforcement learning and oak offer the first plausible mechanistic answer to
58:39
several important questions. How can high level knowledge be learned from low-level experience? Where do concepts
58:45
come from? How do we reason? What is reason? Perhaps reason is just planning
58:51
in this way. What is the purpose of play to find these these uh subpros which
58:58
structure our our cognitive um our cognition? And what is the purpose of
59:03
perception? We're answering the question of how we perception can operate without reference to a a human label or to an
59:11
external world. Perception um can be concepts that have been formed to solve
59:17
problems that are the basis of uh subpros. And if you know about cognitive
59:26
uh David Maher, then we would say that oak is a computational theory of intelligence.
59:33
Okay. Now, what about someone like yourself, a reinforcement learning AI scientist? I I would hope that you would
59:39
think that Oak provides a way to think about the parts of AI AI and their interaction and this can guide future
59:45
research. It's a vision for how to do planning with a learned model which is a key missing ability for today's AIS. Uh
59:52
it offers a view of perception that's grounded in experience rather than in human labels. It offers incomplete
1:00:01
admittedly incomplete but schematic answers to the discovery problem. Where do the subpros, options and features
1:00:07
come from? So it's a vision of how we can obtain an open-ended super
1:00:13
intelligence entirely grown from experience. uh even if it's not yet
1:00:18
fully specified and e even if there are things that I'm saying we don't really know how to do but we should know how to
1:00:24
do such as solve continual learning and metalarning um it's a vision of how to grow a super
1:00:31
intelligence from experience at runtime his most important capabilities and does it act learn plan model learning sub
1:00:39
problems the options with um all the rest and the discovery of of the state
1:00:45
features and thereby of the problems, options and models leading into this
1:00:50
virtuous open-ended cycle of discovery and is completely general and is thus scalable and a potentially lasting
1:00:58
impact. Thank you very much.